{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1070e698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Пример текста с темой:\n",
      "Тема: Библиотека\n",
      "Текст: Штабс-капитан П. Н. Нестеров на днях, увидев в районе Желтиева, в Галиции, летящий над нашим расположением австрийский аэроплан, собиравшийся бросить бомбы, взлетел на воздух, атаковал неприятеля и протаранил неприятельский аппарат, предотвратив жертвы в наших войсках. Сам Нестеров при этом погиб см...\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Общее количество текстов: 803938\n",
      "Количество текстов без темы: 62002\n",
      "\n",
      "Количество уникальных тем: 35\n",
      "Список уникальных тем:\n",
      "['Библиотека' 'Россия' 'Мир' 'Экономика' 'Интернет и СМИ' 'Спорт'\n",
      " 'Культура' 'Из жизни' 'Силовые структуры' 'Наука и техника' 'Бывший СССР'\n",
      " nan 'Дом' 'Сочи' 'ЧМ-2014' 'Путешествия' 'Ценности' 'Легпром' 'Бизнес'\n",
      " 'МедНовости' 'Оружие' '69-я параллель' 'Культпросвет ' 'Крым' 'topic'\n",
      " 'tech' 'politics' 'business' 'entertainment' 'sport' 'cricket' 'tennis'\n",
      " 'football' 'rugby' 'athletics']\n",
      "\n",
      "Распределение по классам:\n",
      "topic\n",
      "Россия               160445\n",
      "Мир                  136621\n",
      "Экономика             79528\n",
      "Спорт                 64413\n",
      "Культура              53797\n",
      "Бывший СССР           53402\n",
      "Наука и техника       53136\n",
      "Интернет и СМИ        44663\n",
      "Из жизни              27605\n",
      "Дом                   21734\n",
      "Силовые структуры     19596\n",
      "Ценности               7766\n",
      "Бизнес                 7399\n",
      "Путешествия            6408\n",
      "69-я параллель         1268\n",
      "Крым                    666\n",
      "sport                   511\n",
      "business                510\n",
      "politics                417\n",
      "tech                    401\n",
      "entertainment           386\n",
      "Культпросвет            340\n",
      "football                265\n",
      "rugby                   147\n",
      "cricket                 124\n",
      "Легпром                 114\n",
      "athletics               101\n",
      "tennis                  100\n",
      "Библиотека               65\n",
      "Оружие                    3\n",
      "ЧМ-2014                   2\n",
      "Сочи                      1\n",
      "topic                     1\n",
      "МедНовости                1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"lenta-ru-news-extended.csv\")\n",
    "\n",
    "text_column = df.iloc[:, 2]  \n",
    "topic_column = df.iloc[:, 3]  \n",
    "\n",
    "print(\"Пример текста с темой:\")\n",
    "print(f\"Тема: {topic_column[3]}\")\n",
    "print(f\"Текст: {text_column[2][:300]}...\") \n",
    "print(\"\\n\" + \"-\" * 50 + \"\\n\")\n",
    "\n",
    "total_samples = len(df)\n",
    "print(f\"Общее количество текстов: {total_samples}\")\n",
    "\n",
    "miss_topic = topic_column.isnull().sum()\n",
    "if miss_topic == 0:\n",
    "    print(\"Все тексты имеют тему.\")\n",
    "else:\n",
    "    print(f\"Количество текстов без темы: {miss_topic}\")\n",
    "\n",
    "unique_topics = topic_column.unique()\n",
    "num_topic = len(unique_topics)\n",
    "print(f\"\\nКоличество уникальных тем: {num_topic}\")\n",
    "print(\"Список уникальных тем:\")\n",
    "print(unique_topics)\n",
    "\n",
    "class_distribution = topic_column.value_counts()\n",
    "print(\"\\nРаспределение по классам:\")\n",
    "print(class_distribution)\n",
    "\n",
    "class_distribution.to_csv(\"class_distribution.csv\", index=True, header=[\"count\"], encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2961955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Пример текста с темой:\n",
      "Тема: Россия\n",
      "Текст: Как стало известно агентству Ассошиэйтед Пресс, экипаж последней экспедиции на станцию \"Мир\" считает ее способной выйти из-под контроля.Командир Виктор Афанасьев сказал: \"Мы чувствуем себя хорошо, но грустим, что оставили станцию летать в беспилотном режиме.\" Gazeta Ru подробно писала о том, что фин...\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Общее количество текстов: 736508\n",
      "Количество текстов без темы: 0\n",
      "\n",
      "Количество уникальных тем: 14\n",
      "Список уникальных тем:\n",
      "['Россия' 'Мир' 'Экономика' 'Интернет и СМИ' 'Спорт' 'Культура' 'Из жизни'\n",
      " 'Силовые структуры' 'Наука и техника' 'Бывший СССР' 'Дом' 'Путешествия'\n",
      " 'Ценности' 'Бизнес']\n",
      "\n",
      "Распределение по классам:\n",
      "topic\n",
      "Россия               160442\n",
      "Мир                  136620\n",
      "Экономика             79528\n",
      "Спорт                 64413\n",
      "Культура              53796\n",
      "Бывший СССР           53402\n",
      "Наука и техника       53136\n",
      "Интернет и СМИ        44663\n",
      "Из жизни              27605\n",
      "Дом                   21734\n",
      "Силовые структуры     19596\n",
      "Ценности               7766\n",
      "Бизнес                 7399\n",
      "Путешествия            6408\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"lenta-ru-news-extended.csv\")\n",
    "min_samples_per_class = 5000\n",
    "\n",
    "df = df.dropna(subset=[df.columns[2], df.columns[3]])\n",
    "class_counts = df.iloc[:, 3].value_counts()\n",
    "valid_classes = class_counts[class_counts >= min_samples_per_class].index\n",
    "df = df[df.iloc[:, 3].isin(valid_classes)]\n",
    "df = df[df.iloc[:, 3] != 'nan']\n",
    "df = df[df.iloc[:, 3].astype(str) != 'nan']\n",
    "\n",
    "text_column = df.iloc[:, 2]\n",
    "topic_column = df.iloc[:, 3]\n",
    "\n",
    "df.iloc[:, 3] = df.iloc[:, 3].astype(str).str.strip()\n",
    "df = df.dropna(subset=[df.columns[3]])\n",
    "\n",
    "class_counts = df.iloc[:, 3].value_counts()\n",
    "valid_classes = class_counts[class_counts >= min_samples_per_class].index\n",
    "df = df[df.iloc[:, 3].isin(valid_classes)]\n",
    "\n",
    "text_column = df.iloc[:, 2]\n",
    "topic_column = df.iloc[:, 3]\n",
    "\n",
    "print(\"Пример текста с темой:\")\n",
    "print(f\"Тема: {topic_column.iloc[0]}\")\n",
    "print(f\"Текст: {text_column.iloc[0][:300]}...\") \n",
    "print(\"\\n\" + \"-\" * 50 + \"\\n\")\n",
    "\n",
    "total_samples = len(df)\n",
    "print(f\"Общее количество текстов: {total_samples}\")\n",
    "\n",
    "miss_topics = topic_column.isnull().sum()\n",
    "print(f\"Количество текстов без темы: {miss_topics}\")\n",
    "\n",
    "unique_topic = topic_column.unique()\n",
    "num_unique_topic = len(unique_topic)\n",
    "print(f\"\\nКоличество уникальных тем: {num_unique_topic}\")\n",
    "print(\"Список уникальных тем:\")\n",
    "print(unique_topic)\n",
    "\n",
    "class_distribution = topic_column.value_counts()\n",
    "print(\"\\nРаспределение по классам:\")\n",
    "print(class_distribution)\n",
    "\n",
    "class_distribution.to_csv(\"class_distribution_cleaned.csv\", index=True, header=[\"count\"], encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b840e72d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 736508/736508 [00:17<00:00, 42090.84it/s]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "texts = df.iloc[:, 2].astype(str).tolist()\n",
    "topic = df.iloc[:, 3].astype(str).tolist()\n",
    "\n",
    "tokenized_texts = [text.lower().split() for text in texts]\n",
    "\n",
    "vocab_counter = Counter()\n",
    "for tokens in tokenized_texts:\n",
    "    vocab_counter.update(tokens)\n",
    "\n",
    "vocab = ['<PAD>', '<UNK>'] + [word for word, count in vocab_counter.most_common(20000)]\n",
    "\n",
    "word2idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "UNK_idx = word2idx['<UNK>']\n",
    "PAD_idx = word2idx['<PAD>']\n",
    "\n",
    "max_len = 128\n",
    "\n",
    "def encode_text(tokens):\n",
    "    return [word2idx.get(word, UNK_idx) for word in tokens[:max_len]]\n",
    "\n",
    "sequences = [encode_text(tokens) for tokens in tqdm(tokenized_texts)]\n",
    "\n",
    "def pad_seq(seq, maxlen):\n",
    "    if len(seq) > maxlen:\n",
    "        return seq[:maxlen]\n",
    "    else:\n",
    "        return seq + [PAD_idx] * (maxlen - len(seq))\n",
    "\n",
    "padded_seq = np.array([pad_seq(seq, max_len) for seq in sequences])\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_topics = label_encoder.fit_transform(topic)\n",
    "num_classes = len(label_encoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d966bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_temp, y_train, y_temp = train_test_split(padded_seq, encoded_topics, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2309efcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            torch.tensor(self.texts[idx], dtype=torch.long),\n",
    "            torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        )\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "train_dataset = TextDataset(X_train, y_train)\n",
    "val_dataset = TextDataset(X_val, y_val)\n",
    "test_dataset = TextDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "126b710e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=128, hidden_dim=128, num_classes=14, num_layers=1, bidirectional=False):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=PAD_idx)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, bidirectional=bidirectional, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim * (2 if bidirectional else 1), num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        lstm_out, (h_n, _) = self.lstm(x)\n",
    "        out = self.fc(h_n[-1])\n",
    "        return out\n",
    "\n",
    "model = LSTMClassifier(len(vocab), num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae942d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|██████████| 8056/8056 [00:41<00:00, 193.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 7684.9247, Val Acc: 0.8019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|██████████| 8056/8056 [00:41<00:00, 196.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 4239.6994, Val Acc: 0.8168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|██████████| 8056/8056 [00:41<00:00, 195.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: 3579.5629, Val Acc: 0.8209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: 100%|██████████| 8056/8056 [00:40<00:00, 197.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Loss: 3074.1761, Val Acc: 0.8204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: 100%|██████████| 8056/8056 [00:42<00:00, 191.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Loss: 2605.0533, Val Acc: 0.8170\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "def evaluate(loader):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for texts, labels in loader:\n",
    "            texts, labels = texts.to(device), labels.to(device)\n",
    "            outputs = model(texts)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return correct / total\n",
    "\n",
    "num_epochs = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for texts, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "        texts, labels = texts.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(texts)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    val_acc = evaluate(val_loader)\n",
    "    print(f\"Epoch {epoch+1}, Loss: {running_loss:.4f}, Val Acc: {val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2750ad16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "class BiLSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=300, hidden_dim=256, num_classes=14, num_layers=2, dropout=0.5):\n",
    "        super(BiLSTMClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers,\n",
    "                            bidirectional=True, batch_first=True, dropout=dropout)\n",
    "        self.batch_norm = nn.BatchNorm1d(hidden_dim * 2)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)  \n",
    "        lstm_out, (h_n, _) = self.lstm(x) \n",
    "        out = lstm_out[:, -1, :] \n",
    "        out = self.batch_norm(out)\n",
    "        out = self.dropout(out)\n",
    "        logits = self.fc(out)\n",
    "        return logits\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d47872",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "np.save('classes.npy', label_encoder.classes_)\n",
    "with open('word2idx.pkl', 'wb') as f:\n",
    "    pickle.dump(word2idx, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fcd497",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score\n",
    "import copy\n",
    "\n",
    "\n",
    "def train_model(model, dataset, val_loader, batch_size=64, epochs=5, patience=3, verbose=True):\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=1)\n",
    "\n",
    "    best_val_acc = 0\n",
    "    best_model_weights = None\n",
    "    no_improvement = 0\n",
    "\n",
    "    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "\n",
    "    dataset_size = len(dataset)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        indices = torch.randperm(dataset_size)\n",
    "        sampler = SubsetRandomSampler(indices)\n",
    "\n",
    "        train_loader = DataLoader(dataset, batch_size=batch_size, sampler=sampler)\n",
    "\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        all_preds, all_labels = [], []\n",
    "\n",
    "        for texts, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "            texts, labels = texts.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(texts)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        train_acc = accuracy_score(all_labels, all_preds)\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss, val_preds, val_true = 0, [], []\n",
    "        with torch.no_grad():\n",
    "            for texts, labels in val_loader:\n",
    "                texts, labels = texts.to(device), labels.to(device)\n",
    "                outputs = model(texts)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                preds = torch.argmax(outputs, dim=1)\n",
    "                val_preds.extend(preds.cpu().numpy())\n",
    "                val_true.extend(labels.cpu().numpy())\n",
    "\n",
    "        val_acc = accuracy_score(val_true, val_preds)\n",
    "        val_loss = val_loss / len(val_loader)\n",
    "\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        history['train_loss'].append(avg_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Epoch {epoch+1}: \"\n",
    "                  f\"Train Loss: {avg_loss:.4f}, Train Acc: {train_acc:.4f} | \"\n",
    "                  f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_model_weights = copy.deepcopy(model.state_dict())\n",
    "            torch.save(best_model_weights, f\"best_bilstm_epoch{epoch+1}_acc{val_acc:.4f}.pth\")\n",
    "            no_improvement = 0\n",
    "        else:\n",
    "            no_improvement += 1\n",
    "\n",
    "        if no_improvement >= patience:\n",
    "            print(\"stopping triggered.\")\n",
    "            break\n",
    "\n",
    "    print(f\"Best val accuracy: {best_val_acc:.4f}\")\n",
    "    return history\n",
    "\n",
    "\n",
    "model = BiLSTMClassifier(vocab_size=len(vocab), num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6d6f45c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:   0%|          | 0/8056 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|██████████| 8056/8056 [03:59<00:00, 33.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 0.4659, Train Acc: 0.8405 | Val Loss: 0.4840, Val Acc: 0.8300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|██████████| 8056/8056 [03:59<00:00, 33.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss: 0.4308, Train Acc: 0.8509 | Val Loss: 0.4727, Val Acc: 0.8368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|██████████| 8056/8056 [03:50<00:00, 34.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Loss: 0.4012, Train Acc: 0.8600 | Val Loss: 0.4822, Val Acc: 0.8356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: 100%|██████████| 8056/8056 [03:53<00:00, 34.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Loss: 0.3727, Train Acc: 0.8700 | Val Loss: 0.4831, Val Acc: 0.8349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: 100%|██████████| 8056/8056 [04:02<00:00, 33.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train Loss: 0.2354, Train Acc: 0.9179 | Val Loss: 0.5421, Val Acc: 0.8391\n",
      "Best validation accuracy: 0.8391\n"
     ]
    }
   ],
   "source": [
    "history = train_model(\n",
    "    model=model,\n",
    "    dataset=train_dataset,\n",
    "    val_loader=val_loader,\n",
    "    batch_size=64,\n",
    "    epochs=5,\n",
    "    patience=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b18c04c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Существующие темы:\n",
      "['Россия' 'Мир' 'Экономика' 'Интернет и СМИ' 'Спорт' 'Культура' 'Из жизни'\n",
      " 'Силовые структуры' 'Наука и техника' 'Бывший СССР' 'Дом' 'Путешествия'\n",
      " 'Ценности' 'Бизнес']\n",
      "\n",
      "Модель загружена. Введенный текст:\n",
      "дома кипел чайник. он стоял рядом с креслом, которое досталось ему от бабушки - тоже прошло войну. через пару лет ему вручали нобелевскую премию\n",
      "Предсказанная тема: Из жизни\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "class BiLSTMClassifier(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=300, hidden_dim=256, num_classes=14, num_layers=2, dropout=0.5):\n",
    "        super(BiLSTMClassifier, self).__init__()\n",
    "        self.embedding = torch.nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.lstm = torch.nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers,\n",
    "                                 bidirectional=True, batch_first=True, dropout=dropout)\n",
    "        self.batch_norm = torch.nn.BatchNorm1d(hidden_dim * 2)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        self.fc = torch.nn.Linear(hidden_dim * 2, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        out = lstm_out[:, -1, :]\n",
    "        out = self.batch_norm(out)\n",
    "        out = self.dropout(out)\n",
    "        logits = self.fc(out)\n",
    "        return logits\n",
    "\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    tokens = re.sub(r\"[^а-яА-ЯёЁa-zA-Z0-9 ]\", \" \", text).split()\n",
    "    return tokens\n",
    "\n",
    "\n",
    "with open(\"word2idx.pkl\", \"rb\") as f:\n",
    "    word2idx = pickle.load(f)\n",
    "\n",
    "label_encoder = np.load(\"classes.npy\", allow_pickle=True)\n",
    "\n",
    "\n",
    "def predict_topic(model, text, word2idx, label_encoder, max_len=128):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.eval()\n",
    "\n",
    "    tokens = preprocess(text)\n",
    "    sequence = [word2idx.get(word, word2idx['<UNK>']) for word in tokens]\n",
    "    sequence = sequence[:max_len] + [word2idx['<PAD>']] * max(0, max_len - len(tokens))\n",
    "\n",
    "    input_tensor = torch.tensor([sequence], dtype=torch.long).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "        predicted_idx = torch.argmax(output, dim=1).item()\n",
    "\n",
    "    return label_encoder[predicted_idx]\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = BiLSTMClassifier(vocab_size=len(word2idx), num_classes=len(label_encoder))\n",
    "model.load_state_dict(torch.load(\"best_bilstm_epoch5_acc0.8391.pth\", map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Существующие темы:\")\n",
    "    print(unique_topic)\n",
    "    print(\"\\nМодель загружена. Введенный текст:\")\n",
    "    user_input = \"дома кипел чайник. он стоял рядом с креслом, которое досталось ему от бабушки - тоже прошло войну. через пару лет ему вручали нобелевскую премию\"\n",
    "    predicted_topic = predict_topic(model, user_input, word2idx, label_encoder)\n",
    "    print(user_input)\n",
    "    print(f\"Предсказанная тема: {predicted_topic}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f6ae76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
